---
title: "Exercise 1"
author: "Adam Sulak, Onder Akacik, Arda Ergin"
date: "2024-02-22"
output:
  pdf_document: default
  html_document: default
---

# Exercise 2

```{r}
data <- read.csv("Birthweight.csv")
```

## Question 2a)

```{r}
model_full <- lm(Birthweight ~ ., data = data)
```

In order to investigate "the problem of potential and influence points", we can first take a look at the diagnostic plots.

```{r}
par(mfrow=c(2,2))
plot(model_full, which=1)
plot(model_full, which=2)
plot(model_full, which=4)
plot(model_full, which=5)
```

Overall, both based on **Cook's distance** plots reveal that cases $20$, $24$, and $28$ appear to be potential outliers.

**Investigating multi-collinearity**:
```{r}
car::vif(model_full)
```

When we look into `car::vif()`, if we follow the rule of thumb of "VIF values should be lower than $5$", we can see that the variables of `fage` and `mage` appears to be a cause for concern regarding multi-collinearity. The other variables do not seem to have an issue.


## Question 2b)

```{r, include = FALSE}
# step_down_model <- step(model, direction = 'backward')
model <- lm(
    Birthweight ~ Length + Headcirc + Gestation + mage + mnocig + mheight + mppwt + fage + fedyrs + fnocig + fheight,
    data = data
)
# remove fage
model_1 <- lm(
    Birthweight ~ Length + Headcirc + Gestation + mage + mnocig + mheight + mppwt + fedyrs + fnocig + fheight,
    data = data
)
# remove mheight
model_2 <- lm(
    Birthweight ~ Length + Headcirc + Gestation + mage + mnocig + mppwt + fedyrs + fnocig + fheight,
    data = data
)
# remove fedyrs
model_3 <- lm(
    Birthweight ~ Length + Headcirc + Gestation + mage + mnocig + mppwt + fnocig + fheight,
    data = data
)
# remove fnocig as it's not significant and Adjusted R-squared is comparable
model_4 <- lm(
    Birthweight ~ Length + Headcirc + Gestation + mage + mnocig + mppwt + fheight,
    data = data
)

# remove mnocig as it's not significant and Adjusted R-squared is comparable
model_5 <- lm(
    Birthweight ~ Length + Headcirc + Gestation + mage + mppwt + fheight,
    data = data
)

# remove length as it's not significant and Adjusted R-squared is comparable
model_6 <- lm(
    Birthweight ~ Headcirc + Gestation + mage + mppwt + fheight,
    data = data
)

# remove fheight as it's not significant and Adjusted R-squared is comparable
model_7 <- lm(
    Birthweight ~ Headcirc + Gestation + mage + mppwt,
    data = data
)

# remove mage as it's not significant and Adjusted R-squared is comparable
model_8 <- lm(
    Birthweight ~ Headcirc + Gestation + mppwt,
    data = data
)


# remove mppwt as it's not significant and Adjusted R-squared is comparable
step_down_model <- lm(
    Birthweight ~ Headcirc + Gestation,
    data = data
)
```

We have followed the procedure for the **step down method** (code omitted for length reasons). We started with the full model, and one by one removed the variable with the largest $p$-value if $p > 0.05$. Removal order: `fage`, `mheight`, `fedyrs`, `fnocig`, `mnocig`, `length`, `fheight`, `mage`, `mppwt`. The final model we end up with is the following:
```{r}
step_down_model <- lm(Birthweight ~ Headcirc + Gestation, data = data)
```

For this model, we have checked the model **assumptions**. First, we can check the assumption of **linearity** through creating scatterplots for $(x,y)$ variable combinations. The scatterplots show that both of the IVs have a linear relationship with the `Birthweight` DV. 
```{r}
pairs(~Birthweight + Headcirc + Gestation, data = data)
```

We can further check the **normality of the errors** through the diagnostic plots. The "Residuals vs. Fitted" plot shows that the errors seem to be equally distributed across the fitted values, and it does not look like there is a clear violation of the assumption.
```{r}
plot(step_down_model, which=1)
```

## Question 2c)
```{r}
head_mean <- mean(data$Gestation)
gest_mean <- mean(data$Headcirc)

df <- data.frame(Gestation = gest_mean, Headcirc = head_mean)

ci <- predict(step_down_model, newdata = df, interval = 'confidence')
pi <- predict(step_down_model, newdata = df, interval = 'prediction')

ci; pi
```

## Question 2d)

In order to asses accuracy of LASSO method we create training and testing data sets. Then we use training data to fit the model using LASSO method (alpha = 1) and then determine optimal lambda value by performing cross-validation. We then compare resulting MSE values to determine if model is better than the one obtained by step-down method.

```{r}
x <- subset(data, select = -Birthweight)
y <- data$Birthweight

train=sample(1:nrow(x),0.67*nrow(x))
x_train=x[train,]; y_train=y[train]
x_test=x[-train,]; y_test = y[-train]

y_predict_lm=predict(step_down_model,newdata=data[-train,]) # predict for the test rows
mse_lm=mean((y_test-y_predict_lm)^2)# prediction quality by the linear model

lasso_model <- glmnet(x_train, y_train, alpha = 1)

lasso_cv <- cv.glmnet(
    as.matrix(x_train),
    y_train,
    alpha = 1,
    type.measure="mse",
    nfolds=5
)

lasso_pred1=predict(lasso_model,s=lasso_cv$lambda.min,newx=as.matrix(x_test))
lasso_pred2=predict(lasso_model,s=lasso_cv$lambda.1se,newx=as.matrix(x_test))
mse1_lasso=mean((y_test-lasso_pred1)^2)
mse2_lasso=mean((y_test-lasso_pred2)^2)


plot(lasso_model, xvar="lambda", label = TRUE)
plot(lasso_model, xvar="dev", label = TRUE)
plot(lasso_cv)
coef(lasso_cv, s= lasso_cv$lambda.1se)
# coef(lasso_cv_model, s= lasso_cv_model$lambda.min)

mse_lm; mse1_lasso; mse2_lasso
```

After running experiment multiple times we determine that step-down model performs better with average MSE value of 0.15 compared to LASSO model with lambda = lambda.1se which had average MSE value of 0.2.

## Question 2e)

```{r}
data_new <- data[,c("lowbwt","Gestation","smoker","mage35")]
data_new$Gestation <- as.numeric(data_new$Gestation)
data_new$smoker <- factor(data_new$smoker, levels = 0:1, labels = c("No","Yes"))
data_new$mage35 <- factor(data_new$mage35, levels = 0:1, labels = c("No","Yes"))
```

To investigate "Do *smoking mothers* seem to have lighter babies?" and "Do *older mothers* seem to have lighter babies?", we can check the **Crosstabs**:

```{r}
xtabs(lowbwt~smoker+mage35,data=data_new) / xtabs(~smoker+mage35,data=data_new)
# Aggregate over mage35 and smoker
xtabs(lowbwt~smoker,data=data_new) / xtabs(~smoker,data=data_new)
xtabs(lowbwt~mage35,data=data_new) / xtabs(~mage35,data=data_new)
```

```{r}
# Factorizing the DV after the crosstabs:
data_new$lowbwt <- factor(data_new$lowbwt, levels = 0:1, labels = c("No","Yes"))
```

To get an even more visual look, we can further plot these proportion of low birth weight by *Smoking* and *Mother Age*.

```{r}
aggregated_data <- data %>%
  group_by(smoker, mage35) %>%
  summarise(proportion_dv = mean(lowbwt), .groups = 'drop')
aggregated_data$smoker <- factor(aggregated_data$smoker, levels = 0:1, labels = c("No","Yes"))
aggregated_data$mage35 <- factor(aggregated_data$mage35, levels = 0:1, labels = c("No","Yes"))

ggplot(aggregated_data, aes(x = mage35, y = proportion_dv, fill = as.factor(smoker))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Mother Over 35 Years Old", y = "Proportion of Low Birth Weight", fill = "Smoker",
       title = "Proportion of Low Birth Weight (< 6 lbs) by Smoking and Mother Age") +
  theme_classic(base_family = "Times") + scale_fill_brewer(palette = color_choice) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 15))
```

Based on the crosstabs and the bar plot:\
1) It is immediately clear that there is *a higher proportion of low birth weight* in babies given birth by smoking mothers, compared to babies given birth by non-smoker mothers. If we aggregate over `mage35` variable in crosstabs, we can see that the difference between the proportions of $0.05$ (non-smoker) and $0.23$ (smoker) appears to be quite considerable. Hence, smoking mothers indeed seem to have lighter babies.\
2) It appears "older mothers seem to have lighter babies", however the *main effect* is not as apparent as being a smoker.\
- **Within the smoker mothers**, age of the mother seems to influence low birth weight: compared to smoker mothers that are younger than $35$, smoker mothers that are $35$ or older seem to have a higher proportion of Low Birth Weight in babies.\
- **Within the non-smoker mothers**, age of the mother does not seem to influence low birth weight: there appears to be no difference between non-smoker mothers that are younger than $35$ and non-smoker mothers that are $35$ or older in Low Birth Weight in babies (since the difference in proportions of $0.05$ and $0$ seem negligible). - If we aggregate over `smoker` variable in crosstabs, we can see that there seems to be a difference between the proportions of $0.13$ (under $35$ y.o.) and $0.25$ (over $35$ y.o.). Yet, we need further testing to see whether this difference is actually significant. Although, considering the above points, it is possible that there is a `smoker*mage35` interaction.

## Question 2f)

```{r}
model_glm_0 <- stats::glm(lowbwt ~ 1, family=binomial("logit"), data=data_new)
model_glm_smoker <- stats::glm(lowbwt ~ smoker, family=binomial("logit"), data=data_new)
model_glm_mage35 <- stats::glm(lowbwt ~ mage35, family=binomial("logit"), data=data_new)
model_glm_both <- stats::glm(lowbwt ~ smoker + mage35, family=binomial("logit"), data=data_new)
# Testing the Predictors through model comparison:
results_1 <- anova(model_glm_0, model_glm_smoker, test="Chisq")
results_2 <- anova(model_glm_0, model_glm_mage35, test="Chisq")
# Odds
exp(model_glm_smoker$coefficients[2])
exp(model_glm_mage35$coefficients[2])
```

The results of the binomial logistic regression model, with `glm`, shows that neither of the predictors have a significant main effect. We can see this through testing each of the predictors with making model comparison with `anova(model_1, model_2, test="Chisq")`.

We compared a base model, which includes only the intercept, to models including whether the mother is a smoker (`smoker`), age of the mother (`mage35`), and both of these predictors predictors (`smoker + mage35`) to assess the significance of each predictor. The analysis of deviance using Chi-square tests revealed that neither of the predictors are significant in their effect on low birth weight for newborns (`lowbwt`).

For the comparison between the base model and the model including `smoker`, there was no significant improvement in model prediction, $Deviance = 2.93$, $p = .087$. Similarly, for the comparison between the base model and the model including `mage35`, there was no significant improvement in model prediction, $Deviance = 0.35$, $p = .55$.

Although, it needs to be noted that the predictor of `smoker` is very close to significance with $p = .087$, which is in line with the graphs we have produced for 2e, although we would have expect it to be significant. Regarding `mage35`, based on the graphs, we had doubts whether this was a significant predictor, and now we can see that it is not.

When we investigate the **odds**, we can take the exponential of the coefficients in the model. The results of the models show that the odds of having a low birth weight baby for smoker mothers are about 5.6 times the odds for non-smoker mothers. And, the odds of having a low birth weight baby for mothers older than $35$ are about 2.2 times the odds for mothers younger than $35$. Although, as stated above, neither of these predictors seem to be significant with the default alpha level of $0.05$.

## Question 2g)

Investigate the interaction of predictor *Gestation* with *smoker*, and the interaction of *Gestation* with *mage35* (one interaction at a time). From this and f), choose a resulting model.

```{r}
model_glm_Gestation <- stats::glm(lowbwt ~ Gestation, family=binomial("logit"), data=data_new)
model_glm_int_smoker <- stats::glm(lowbwt ~ smoker*Gestation, family=binomial("logit"), data=data_new)
model_glm_int_mage35 <- stats::glm(lowbwt ~ mage35*Gestation, family=binomial("logit"), data=data_new)

# Testing with ANOVA
results_1 <- anova(model_glm_0, model_glm_Gestation, test="Chisq")
results_2 <- anova(model_glm_Gestation, model_glm_int_smoker, test = "Chisq")
results_3 <- anova(model_glm_Gestation, model_glm_int_mage35, test = "Chisq")
```

We followed a similar model comparison procedure as 2f. As a first step, we can see that the model with `Gestation` as a predictor, compared to only the model with the intercept is significantly better in terms of model prediction, $Deviance = 16.4$, $p < .001$.

Given that the models with `smoker` and `mage35` predictors are not significantly better than the model with only the intercept, when we add the interaction terms (seperately to two models), we can deduce whether the interaction terms `smoker*Gestation` and `mage35*Gestation` provide significant improvement to model prediction.

We can see that the `smoker*Gestation` model is significantly better than just having `Gestation` as a predictor, $Deviance = 7.87$, $p = .02$. However, the model with `mage35*Gestation` model is not a significant improvement than just having `Gestation` as a predictor.

To note, running this `glm` model gives a `Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred`. Upon some research, we have discovered that this warning means that the model almost perfectly predict the outcome variable. We interpret this as a good sign.

However, we need to consider that the model improvement for `lowbwt ~ smoker * Gestation` might simply depend on the near-significant effect (see our answer for 2f) of `smoker` as a predictor. So, we can further test this by running a Chi-square test for the interaction model.

```{r}
results <- anova(model_glm_int_smoker, test = "Chisq")
```

We can see that the interaction term `smoker:Gestation` is not significant, $p = 0.198$. We also further confirm this result through a model comparison with `smoker + Gestation` and `smoker*Gestation`:

```{r}
model_glm_noint_smoker <- stats::glm(lowbwt ~ smoker + Gestation, family=binomial("logit"), data=data_new)
anova(model_glm_noint_smoker, model_glm_int_smoker, test = "Chisq")
```

Hence, even though `smoker` variable is nearly non-significant, due to the model comparison results, we choose our final model as `lowbwt ~ smoker + Gestation`.

## Question 2h)

h)  For the resulting model from g), estimate the probability of low baby weight for each combination of levels of the involved factors and the gestation of 40 weeks.

```{r}
new_data <- data.frame(Gestation = 40, smoker = factor(c("No", "Yes"), levels = c("No", "Yes")))
probabilities <- predict(model_glm_noint_smoker, newdata = new_data, type = "response")
new_data$probability_lowbwt <- probabilities
print(new_data)

```

## Question 2i)

i)  Another approach to address the questions in e) would be to apply a contingency table test. Implement the relevant test(s). Is this approach wrong? Name both an advantage and a disadvantage of this approach as compared to the one from f).

```{r}
## Chi-squared test for smoking mothers
smoker_table <- table(data_new$smoker, data_new$lowbwt)
chisq_test_smoker <- chisq.test(smoker_table)
chisq_test_smoker

## Chi-squared test for mothers over 35
mage35_table <- table(data_new$mage35, data_new$lowbwt)
chisq_test_mage35 <- chisq.test(mage35_table)
chisq_test_mage35
```
